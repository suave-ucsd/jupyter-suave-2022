{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color:red\">Examples of Spatial Statistics</span></h1>\n",
    "\n",
    "Currently works with polygon data in SuAVE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Retrieve survey parameters from the URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "function getQueryStringValue (key)\n",
    "{  \n",
    "    return unescape(window.location.search.replace(new RegExp(\"^(?:.*[&\\\\?]\" + escape(key).replace(/[\\.\\+\\*]/g, \"\\\\$&\") + \"(?:\\\\=([^&]*))?)?.*$\", \"i\"), \"$1\"));\n",
    "}\n",
    "IPython.notebook.kernel.execute(\"survey_url='\".concat(getQueryStringValue(\"surveyurl\")).concat(\"'\"));\n",
    "IPython.notebook.kernel.execute(\"views='\".concat(getQueryStringValue(\"views\")).concat(\"'\"));\n",
    "IPython.notebook.kernel.execute(\"view='\".concat(getQueryStringValue(\"view\")).concat(\"'\"));\n",
    "IPython.notebook.kernel.execute(\"user='\".concat(getQueryStringValue(\"user\")).concat(\"'\"));\n",
    "IPython.notebook.kernel.execute(\"csv_file='\".concat(getQueryStringValue(\"csv\")).concat(\"'\")); \n",
    "IPython.notebook.kernel.execute(\"dzc_file='\".concat(getQueryStringValue(\"dzc\")).concat(\"'\")); \n",
    "IPython.notebook.kernel.execute(\"params='\".concat(getQueryStringValue(\"params\")).concat(\"'\")); \n",
    "IPython.notebook.kernel.execute(\"active_object='\".concat(getQueryStringValue(\"activeobject\")).concat(\"'\")); \n",
    "IPython.notebook.kernel.execute(\"full_notebook_url='\" + window.location + \"'\"); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"user = \" + str(user))\n",
    "print(\"survey_url = \" + str(survey_url))\n",
    "print(\"views = \" + str(views))\n",
    "print(\"view = \" + str(view))\n",
    "print(\"csv_file = \" + str(csv_file))\n",
    "print(\"dzc_file = \" + str(dzc_file))\n",
    "print(\"params = \" + str(params))\n",
    "print(\"active_object = \" + str(active_object))\n",
    "print(\"full_notebook_url = \" + str(full_notebook_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# common imports\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "    \n",
    "import numpy as np\n",
    "import panel as pn\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "absolutePath = \"../../temp_csvs/\"\n",
    "\n",
    "# local imports\n",
    "import sys\n",
    "sys.path.insert(1, '../../helpers')\n",
    "import panel_libs as panellibs\n",
    "import suave_integration as suaveint\n",
    "\n",
    "# specific imports\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "import os\n",
    "\n",
    "import libpysal as ps\n",
    "from mgwr.gwr import GWR, MGWR\n",
    "from mgwr.sel_bw import Sel_BW\n",
    "import geopandas as gp\n",
    "import matplotlib as mpl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read the survey file and extract numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read the csv file\n",
    "df = panellibs.extract_data(absolutePath + csv_file)# print(absolutePath + csv_file)\n",
    "\n",
    "# create a list of variable names\n",
    "variables_df = pd.DataFrame({'varname':df.columns})\n",
    "printmd(\"<b><span style='color:red'>All variables in the survey file:</span></b>\")\n",
    "\n",
    "col = 0\n",
    "for var in variables_df.varname.values:\n",
    "    print(str(col) +\":  \"+ var)\n",
    "    col = col+1\n",
    "\n",
    "\n",
    "# create a dictionary of #number variables with abbreviated and full variable names \n",
    "var_list = {n[:n.index('#')]:n for n in variables_df.varname.values if '#number' in n}\n",
    "printmd(\"<b><span style='color:red'>Numeric variables:</span></b>\")\n",
    "\n",
    "col = 0\n",
    "for var in var_list:\n",
    "    print(str(col) +\":  \"+ var)\n",
    "    col = col+1\n",
    "\n",
    "#create a dataframe of only #number variables\n",
    "nums_df = df[[n for n in variables_df.varname.values if '#number' in n]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a geodataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. figure out the geometry field\n",
    "geometry_vars = [col for col in df.columns if 'geometry' in col]\n",
    "geometry_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. convert this field to shapely geometry\n",
    "s = gp.GeoSeries.from_wkt(df[geometry_vars[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. append the geometry series to the dataframe to create a gdf\n",
    "gdf = gp.GeoDataFrame(df, crs=\"EPSG:4326\", geometry=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. make sure it worked Ok, by plotting some variable\n",
    "\n",
    "col_to_map = 'Subjective wellbeing (0-10)#number'\n",
    "gdf.plot(figsize=(15,5), column=col_to_map,legend=True)\n",
    "print (col_to_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ESDA and Spatial Autocorrelation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import esda # from PySAL\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General info about the dataset\n",
    "gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_columns\", None):\n",
    "    display(gdf.drop(['geometry'],axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Explore missing values\n",
    "\n",
    "with pd.option_context(\"display.max_rows\", None):\n",
    "    display(df.isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map selected variable\n",
    "\n",
    "cntry_name = gdf.columns[0]\n",
    "\n",
    "gdf.explore(column=col_to_map, cmap =\"Blues\", scheme=\"FisherJenks\", tiles = \"Stamen Watercolor\", \n",
    "                 tooltip={cntry_name,col_to_map}, popup=True,k=5, highlight=True,\n",
    "                 width=\"100%\", legend_kwds={\"caption\":col_to_map, \"colorbar\":False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a histogram\n",
    "\n",
    "gdf[col_to_map].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explore weights and compute spatial similarity\n",
    "\n",
    "### 5.1 Contiguity-based weights\n",
    "\n",
    "\n",
    "A commonly-used type of weight is a queen contigutiy weight, which reflects adjacency relationships as a binary indicator variable denoting whether or not a polygon shares an edge or a vertex with another polygon. These weights are symmetric, in that when polygon $A$ neighbors polygon $B$, both $w_{AB} = 1$ and $w_{BA} = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wq =  ps.weights.Queen.from_dataframe(gdf)\n",
    "\n",
    "# In addition to \"queen\" weights, we can also compute \"rook\" and \"bishop\" weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the weights \n",
    "from splot.libpysal import plot_spatial_weights\n",
    "plot_spatial_weights(wq, gdf, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use this operation to find neighbors of any country\n",
    "\n",
    "# e.g., neighbors of Afghanistan:\n",
    "selected_country = 0\n",
    "wq.neighbors[selected_country]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in wq.neighbors[selected_country]:\n",
    "    print(gdf[gdf.columns[0]][country])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Distance-based weights\n",
    "\n",
    "Neighborhood relationships can be also defined by distance, not just by adjacency (contiguity). Indeed, in many cases distance-based neighbors make more sense.\n",
    "\n",
    "\"K-nearest neighbor weights\": distance between centroids of polygons is used to compute binary weights (1 if neighbor, 0 if not), By comparison, \"Kernel weights\" return some numeric function of the distance between polygon centroids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Spatial Autocorrelation ##\n",
    "\n",
    "The concept of *spatial autocorrelation* relates to the combination of two types of similarity: spatial\n",
    "similarity and attribute similarity. Although there are many different measures\n",
    "of spatial autocorrelation, they all combine these two types of simmilarity into\n",
    "a summary measure.\n",
    "\n",
    "### 6.1 Spatial Similarity ###\n",
    "\n",
    "In spatial autocorrelation analysis, the spatial weights\n",
    "are used to formalize the notion of spatial similarity. There\n",
    "are many ways to define spatial weights, e.g. using queen contiguity as above.\n",
    "\n",
    "### 6.2 Attribute Similarity ###\n",
    "\n",
    "The spatial weight between neighborhoods $i$ and $j$ indicates if the two \n",
    "are neighbors (i.e., geographically similar). What we also need is a measure of\n",
    "attribute similarity to pair up with this concept of spatial similarity. The\n",
    "**spatial lag** is a derived variable that accomplishes this for us. For neighborhood\n",
    "$i$ the spatial lag is defined as: $$ylag_i = \\sum_j w_{i,j} y_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example, for the variable we explored above (col_to_map):\n",
    "\n",
    "gdfn = gdf.dropna(subset=[col_to_map])\n",
    "y = gdfn[col_to_map]\n",
    "wq =  ps.weights.Queen.from_dataframe(gdfn)\n",
    "wq.transform = 'r'\n",
    "\n",
    "ylag = ps.weights.lag_spatial(wq, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the spatial lag variable, for visual analysis\n",
    "\n",
    "gdfn.explore(column=ylag, cmap =\"Blues\", scheme=\"FisherJenks\", tiles = \"Stamen Watercolor\", \n",
    "                 tooltip={cntry_name,col_to_map}, popup=True,k=5, highlight=True,\n",
    "                 width=\"100%\", legend_kwds={\"caption\":\"Spatial Lag of \" +col_to_map, \"colorbar\":False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Formal statistical assesment of spatial autocorrelation\n",
    "\n",
    "### 7.1 Join counts\n",
    "\n",
    "One way to formalize a test for spatial autocorrelation in a binary attribute is\n",
    "to consider the so-called _joins_. A join exists for each neighbor pair of\n",
    "observations, and the joins are reflected in our binary spatial weights object\n",
    "`wq`. \n",
    "\n",
    "Each unit can take on one of two values \"Black\" or \"White\", analogous to the layout of a chessboard.\n",
    "\n",
    "For a given pair of neighboring locations there are three different types of joins that can\n",
    "arise:\n",
    "\n",
    "- Black Black (BB)\n",
    "- White White (WW)\n",
    "- Black White (or White Black) (BW)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With polygon data, we can conduct an analysis using a contiguity matrix. For our selected variable, we need to first discretize the variable we're using; to keep things simple, we'll binarize it using the median so that \"high\" values are areas with values above the median and \"low\" values are those below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the median\n",
    "y.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create \"low\" and \"high\" values.\n",
    "\n",
    "# those with values above the median\n",
    "yb = y > y.median()\n",
    "sum(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"0 Low\", \"1 High\"]\n",
    "yb = [labels[i] for i in 1*yb] \n",
    "gdfn['yb'] = yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "gdfn.plot(column='yb', cmap='binary', edgecolor='grey', legend=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that we have [sum(yb)] \"black\" polygons. Is the number of Black-Black joins equal to what we would expect if Black polygons were randomply assigned on the map?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using joint counts computation in PySAL:\n",
    "\n",
    "yb = 1 * (y > y.median()) # convert back to binary\n",
    "wq =  ps.weights.Queen.from_dataframe(gdfn)\n",
    "wq.transform = 'b'\n",
    "np.random.seed(12345)\n",
    "jc = esda.join_counts.Join_Counts(yb, wq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jc.bb # Black Black joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jc.ww # White White joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jc.bw # Black White joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this a departure from what we would expect if the process generating the spatial distribution of the Black polygons were a completely random one? \n",
    "\n",
    "To answer this, PySAL uses random spatial permutations of the observed attribute values to generate a realization under the null of complete spatial randomness (CSR). This is repeated a large number of times (999 default) to construct a reference distribution to evaluate the statistical significance of our observed counts.\n",
    "\n",
    "The average number of BB joins from the synthetic realizations is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jc.mean_bb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare it with the observed number of bb joins: is it very different? Is so, we may reject the null hypothesis (complete spatial randomness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jc.p_sim_bb # This the p-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Moran's I\n",
    "\n",
    "This is a test of global spatial autocorrelation for a continues attribute. For join counts, we binarized the variable, now we use the actual values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this computation, make sure we don't have null values\n",
    "\n",
    "gdfn.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recompute y and queen weights\n",
    "y = gdfn[col_to_map]\n",
    "wq =  ps.weights.Queen.from_dataframe(gdfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# regularize the weights (so that sum of weights for each country is 1)\n",
    "\n",
    "wq.transform = 'r'\n",
    "np.random.seed(12345)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = esda.moran.Moran(y, wq)\n",
    "mi.I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splot.esda import plot_moran\n",
    "plot_moran(mi, zstandard=True, figsize=(15,5))\n",
    "plt.show()\n",
    "\n",
    "# On the left: reference distribution vs the observed statistic\n",
    "\n",
    "# On the right: plot of y values against its spatial lag. Moran's I is a slope of this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi.p_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Local spatial autocorrelation\n",
    "\n",
    "Computing hot spots, cold spots, spatial outliers. We've done this before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Spatial Regression\n",
    "\n",
    "From PySAL documentation:\n",
    "\n",
    "The core idea of spatial econometrics is to introduce a formal representation of space into the statistical framework for regression. This can be done in many ways: by including predictors based on space (e.g. distance to relevant features), by splitting the datasets into subsets that map into different geographical regions (e.g. spatial regimes), by exploiting close distance to other observations to borrow information in the estimation (e.g. kriging), or by introducing variables that put in relation their value at a given location with those in nearby locations, to give a few examples. Some of these approaches can be implemented with standard non-spatial techniques, while others require bespoke models that can deal with the issues introduced. \n",
    "\n",
    "### 8.1 Baseline (nonspatial) OLS regression\n",
    "\n",
    "Essentially, the core of a linear regression is to explain a given variable as a linear function of a set of other characteristics we will collectively call $X_i$:\n",
    "\n",
    "$$\n",
    "\\ln(P_i) = \\alpha + \\beta X_i + \\epsilon_i\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will explore our selected variable and explain it based on several additional variables related to health\n",
    "\n",
    "x = [\n",
    "'Women in national parliaments (%)#number',\n",
    "'Science and tech journal articles (items per bn. PPP$ GDP)#number',\n",
    "'Mobile broadband subscriptions (per 100)#number',\n",
    "'Under 5 mortality (per 1000 live births)#number',\n",
    "'Tuberculosis (per 100,000)#number',\n",
    "'Homicides (per 100,000)#number',\n",
    "'Prison population (per 100,000)#number',\n",
    "'Mean years of schooling (years)#number',\n",
    "'Undernourishment (%)#number'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdfn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with missing values\n",
    "gdfn2 = gdfn.dropna(subset=x, how='any')\n",
    "gdfn2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataframe: Scatterplots between X and y; plot the data\n",
    "\n",
    "plt.scatter(x2.values[:,2], y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdfn2.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = gdfn2[x]\n",
    "x2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to compute OLS regression. Here, we'll use the spreg module in PySAL, which implements a standard OLS routine, but is particularly well suited for regressions on spatial data. \n",
    "\n",
    "We'll also build a spatial weights matrix that connects every observation to its 8 nearest neighbors - which results in extra diagnostics from the baseline model though not used in OLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spreg.ols import OLS\n",
    "\n",
    "# We'll also scale the independent variables\n",
    "\n",
    "import sklearn.preprocessing\n",
    "\n",
    "x2s = sklearn.preprocessing.StandardScaler().fit_transform(x2.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = gdfn2[col_to_map]\n",
    "wq =  ps.weights.Queen.from_dataframe(gdfn2)\n",
    "wq.transform = 'R'\n",
    "wq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = OLS(y.values[:, None], x2s, w=wq, spat_diag=True,moran=True,name_x=x2.columns.tolist(),name_y=col_to_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m1.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. Geographically-weighted regression (GWR)\n",
    "\n",
    "(from https://methods.sagepub.com/dataset/howtoguide/geographically-weighted-regression-berlin-districts-2018-python)\n",
    "\n",
    "The fundamental idea behind GWR specifically (and local regression generally) is that the structure of a process being studied may not be the same under all conditions. This concept, sometimes called nonstationarity, is a common facet of cutting-edge models in spatial analysis. It is quite common for effects to vary depending on an observation’s context, group, or relative location. GWR provides a structured way to model this variation.\n",
    "\n",
    "GWR works by creating a dataset that is “local” to each site and running a regression on that site.\n",
    "\n",
    "GWR allows us to recognize that the “overall” effect may not be useful, and that an effect estimated from a more narrow focus on the area around some site i may be different. To build this different estimate, GWR uses an N × N spatial weighting matrix, W.\n",
    "\n",
    "In practice, estimating a GWR requires the model to tradeoff between local and global behavior. In order for the model to capture local variability, Wi must filter out a small set of data around site i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add country centroids\n",
    "\n",
    "gdfn2['X'] = gdfn2.representative_point().x\n",
    "gdfn2['Y'] = gdfn2.representative_point().y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare inputs\n",
    "\n",
    "gdfn2_y = gdfn2[col_to_map].values.reshape((-1,1))\n",
    "gdfn2_X = gdfn2[x].values\n",
    "u = gdfn2['X']\n",
    "v = gdfn2['Y']\n",
    "g_coords = list(zip(u,v))\n",
    "\n",
    "g_X = (gdfn2_X - gdfn2_X.mean(axis=0)) / gdfn2_X.std(axis=0)\n",
    "\n",
    "g_y = gdfn2_y.reshape((-1,1))\n",
    "\n",
    "g_y = (gdfn2_y - gdfn2_y.mean(axis=0)) / gdfn2_y.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calibrate GWR model\n",
    "\n",
    "gwr_selector = Sel_BW(g_coords, g_y, g_X) # selects bandwidth for kernel\n",
    "gwr_bw = gwr_selector.search(bw_min=2) \n",
    "\n",
    "print(gwr_bw) # bandwidth value: a distance or N nearest neighbors\n",
    "gwr_results = GWR(g_coords, g_y, g_X, gwr_bw).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwr_results.params[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwr_results.localR2[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwr_results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diplay variable names \n",
    "\n",
    "i = 1\n",
    "for v in x:\n",
    "    print (str(i) +\":  \"+v)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
