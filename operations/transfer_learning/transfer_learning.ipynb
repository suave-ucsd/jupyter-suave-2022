{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color:red\">SuAVE Transfer Learning Model</span></h1>\n",
    "\n",
    "<h2> WORK IN PROGRESS </h2>\n",
    "Author: Shuyuan Wang\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install imutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Disable autoscroll and retrieve survey parameters from the URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "function getQueryStringValue (key)\n",
    "{  \n",
    "    return unescape(window.location.search.replace(new RegExp(\"^(?:.*[&\\\\?]\" + escape(key).replace(/[\\.\\+\\*]/g, \"\\\\$&\") + \"(?:\\\\=([^&]*))?)?.*$\", \"i\"), \"$1\"));\n",
    "}\n",
    "IPython.notebook.kernel.execute(\"survey_url='\".concat(getQueryStringValue(\"surveyurl\")).concat(\"'\"));\n",
    "IPython.notebook.kernel.execute(\"views='\".concat(getQueryStringValue(\"views\")).concat(\"'\"));\n",
    "IPython.notebook.kernel.execute(\"view='\".concat(getQueryStringValue(\"view\")).concat(\"'\"));\n",
    "IPython.notebook.kernel.execute(\"user='\".concat(getQueryStringValue(\"user\")).concat(\"'\"));\n",
    "IPython.notebook.kernel.execute(\"csv_file='\".concat(getQueryStringValue(\"csv\")).concat(\"'\")); \n",
    "IPython.notebook.kernel.execute(\"dzc_file='\".concat(getQueryStringValue(\"dzc\")).concat(\"'\")); \n",
    "IPython.notebook.kernel.execute(\"params='\".concat(getQueryStringValue(\"params\")).concat(\"'\")); \n",
    "IPython.notebook.kernel.execute(\"active_object='\".concat(getQueryStringValue(\"activeobject\")).concat(\"'\")); \n",
    "IPython.notebook.kernel.execute(\"full_notebook_url='\" + window.location + \"'\"); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import all packages (this might take a few seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import widget functionality\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Markdown, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\"\n",
    "# import the necessary packages\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.utils import to_categorical\n",
    "from imutils import paths\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Dropout\n",
    "from keras import backend as K\n",
    "# import local lenet.py file describing the LeNet implementation with RELU activation functions\n",
    "#from lenet import LeNet\n",
    "\n",
    "# More imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# import the necessary packages for SVM predictor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import imutils\n",
    "\n",
    "\n",
    "# copy from sample\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense,GlobalAveragePooling2D\n",
    "from keras.applications import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initializing the number of times the model will loop and its batch size for learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "epochCount = OrderedDict()\n",
    "epochCount['25 Iterations'] = 25\n",
    "epochCount['50 Iterations'] = 50\n",
    "epochCount['75 Iterations'] = 75\n",
    "\n",
    "def f(epoch_count):\n",
    "    return epoch_count\n",
    "\n",
    "epochNum = interact(f, epoch_count=epochCount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchS = OrderedDict()\n",
    "batchS['32 Batch Size'] = 32\n",
    "batchS['64 Batch Size'] = 64\n",
    "batchS['128 Batch Size'] = 128\n",
    "\n",
    "def f(batch_size):\n",
    "    return batch_size\n",
    "\n",
    "batchNum = interact(f, batch_size=batchS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the number of epochs to train for, init learning rate and batch size\n",
    "EPOCHS = epochNum.widget.result\n",
    "INIT_LR = 1e-3\n",
    "BS = batchNum.widget.result\n",
    "# init the image suffix, yset, and image list\n",
    "suffix = '.png'\n",
    "img_list = []\n",
    "yset = []\n",
    "# create labels list and 2 dicts for 2 way mapping\n",
    "labels = []\n",
    "# key = label value = number\n",
    "label_yval = dict()\n",
    "# key = number value = label\n",
    "yval_label = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load the data and choose the column name to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use csv file to grab images/labels\n",
    "absolutePath = \"../../temp_csvs/\"\n",
    "# read the csv file\n",
    "file = open(absolutePath + csv_file, encoding=\"latin-1\")\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "#generate image path\n",
    "localdzc = dzc_file.replace(\"https://maxim.ucsd.edu/dzgen/lib-staging-uploads\",\"/lib-nfs/dzgen\")\n",
    "full_images_location = localdzc.replace(\"/content.dzc\",\"/full_images/\")\n",
    "\n",
    "\n",
    "# Choose column of label for prediction\n",
    "toPredict = list(df.columns.values)\n",
    "\n",
    "pred_menu = OrderedDict()\n",
    "for i in range(0, len(toPredict)):\n",
    "    pred_menu[toPredict[i]] = toPredict[i]\n",
    "\n",
    "def f(predictions_menu):\n",
    "    return predictions_menu\n",
    "# choose which label for predictions\n",
    "out2 = interact(f, predictions_menu=pred_menu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Select pixel resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = widgets.IntSlider(value=60,min=20,max=300,step=10,description='Size, pixels:')\n",
    "display(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd(\"<h2><span style='color:red'>Verify model parameters:</span></h2>\")\n",
    "printmd(\"<b>Variable to predict:  \" +out2.widget.result + \"</b>\")\n",
    "printmd(\"<b>Pixel resolution:  \" +str(a.value) + \"</b>\")\n",
    "printmd(\"<b>Path to images:  \" +full_images_location + \"</b>\")\n",
    "printmd(\"<b>Number of epochs:  \" + str(EPOCHS) + \"</b>\")\n",
    "printmd(\"<b>Batch size:  \" +str(BS) + \"</b>\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Grab the images and configure them for predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This might take a little while depending on the size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_dimension = a.value\n",
    "# grab chosen column names\n",
    "nameCol = df['#img']\n",
    "predCol = df[out2.widget.result]\n",
    "\n",
    "def matchImage(curr_image_array, image_list):\n",
    "    for i in range(0, len(image_list)):\n",
    "        if (np.array_equal(curr_image_array, image_list[i])):\n",
    "            return i\n",
    "    \n",
    "labels = []\n",
    "# add all fabric columns to the y set\n",
    "for i in range (0,len(predCol)):\n",
    "    labels.append(predCol[i])\n",
    "\n",
    "# grab all unique labels\n",
    "uni_labels = set(labels)\n",
    "uni_labels = list(uni_labels)\n",
    "\n",
    "# assign each label a dict key number\n",
    "for i in range(0,len(uni_labels)):\n",
    "    yval_label[i] = uni_labels[i]\n",
    "    label_yval[uni_labels[i]] = i\n",
    "\n",
    "\n",
    "yset = []    \n",
    "# create list of keys associated with their labels\n",
    "for i in range (0, len(labels)):\n",
    "    yset.append(label_yval[labels[i]])\n",
    "\n",
    "img_list = []\n",
    "file_lst=[]\n",
    "\n",
    "counter = 0\n",
    "im_count = widgets.Label(value=\"0% images loaded\")\n",
    "display(im_count)\n",
    "numfiles = len(nameCol)\n",
    "\n",
    "# gather images from path created from file names in csv file\n",
    "for i in range (0,len(nameCol)):\n",
    "    base_filename = nameCol[i]\n",
    "\n",
    "    fileName = os.path.join(full_images_location, base_filename + suffix)\n",
    "    \n",
    "    file_lst.append(fileName)\n",
    "    im = cv2.imread(fileName)\n",
    "    im = cv2.resize(im, (im_dimension,im_dimension))\n",
    "    im = img_to_array(im)\n",
    "    img_list.append(im)\n",
    "    \n",
    "    counter += 1\n",
    "    im_count.value = str(int(counter / numfiles * 100)) + \"% images loaded\"\n",
    "\n",
    "# Shuffle the data\n",
    "p = np.random.permutation(len(yset))\n",
    "\n",
    "test_train_dict = {}\n",
    "test_train_list = []\n",
    "\n",
    "# Relable for splitting sets\n",
    "Y = []\n",
    "X = []\n",
    "for i in range(0,len(yset)):\n",
    "    Y.append(yset[p[i]])\n",
    "    X.append(file_lst[p[i]])\n",
    "    \n",
    "# split the test and training set 75:25\n",
    "split = int(len(X)*(.75))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf=pd.DataFrame()\n",
    "ndf['id']=file_lst\n",
    "ndf['label']=list(map(str, yset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1/255., validation_split=0.25)\n",
    " \n",
    "train_generator = datagen.flow_from_dataframe(dataframe=ndf, \n",
    "                                             x_col='id',\n",
    "                                             y_col='label',\n",
    "                                             target_size=(224,224),\n",
    "                                              subset='training',\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='binary',\n",
    "                                              \n",
    "                                                 shuffle=True)\n",
    " \n",
    "validation_generator = datagen.flow_from_dataframe(dataframe=ndf, \n",
    "                                             x_col='id',\n",
    "                                             y_col='label',\n",
    "                                             target_size=(224,224),\n",
    "                                                subset='validation',\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='binary',\n",
    "                                                 shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model=ResNet50(weights='imagenet',include_top=False)\n",
    "\n",
    "newOutput=base_model.output\n",
    "newOutput=GlobalAveragePooling2D()(newOutput)\n",
    "newOutput=Dense(1024,activation='relu')(newOutput) \n",
    "newOutput=Dense(1024,activation='relu')(newOutput) \n",
    "newOutput=Dense(512,activation='relu')(newOutput)\n",
    "\n",
    "preds=Dense(2,activation='softmax')(newOutput)\n",
    "\n",
    "\n",
    "model=Model(inputs=base_model.input,outputs=preds)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer = 'Adam',metrics=['accuracy'])\n",
    "\n",
    "train_steps = train_generator.n//train_generator.batch_size\n",
    "validation_steps = validation_generator.n//validation_generator.batch_size\n",
    "\n",
    "#history = model.fit_generator(train_generator,steps_per_epoch=train_steps, epochs=5,\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_generator,steps_per_epoch=train_steps, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model=ResNet50(weights='imagenet',include_top=False)\n",
    "\n",
    "newOutput=base_model.output\n",
    "newOutput=GlobalAveragePooling2D()(newOutput)\n",
    "newOutput=Dense(1024,activation='relu')(newOutput) \n",
    "newOutput=Dense(1024,activation='relu')(newOutput) \n",
    "newOutput=Dense(512,activation='relu')(newOutput)\n",
    "\n",
    "preds=Dense(2,activation='softmax')(newOutput)\n",
    "\n",
    "\n",
    "model=Model(inputs=base_model.input,outputs=preds)\n",
    "\n",
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "step_size_train=train_generator.n//train_generator.batch_size\n",
    "model.fit_generator(generator=train_generator,\n",
    "                   steps_per_epoch=step_size_train,\n",
    "                   epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOP HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_ext(fn):\n",
    "    return fn+\".png\"\n",
    "traindf=pd.read_csv(“./trainLabels.csv”,dtype=str)\n",
    "testdf=pd.read_csv(\"./sampleSubmission.csv\",dtype=str)\n",
    "traindf[\"id\"]=traindf[\"id\"].apply(append_ext)\n",
    "testdf[\"id\"]=testdf[\"id\"].apply(append_ext)\n",
    "datagen=ImageDataGenerator(rescale=1./255.,validation_split=0.25)\n",
    "\n",
    "train_generator=datagen.flow_from_dataframe(\n",
    "dataframe=traindf,\n",
    "directory=\"./train/\",\n",
    "x_col=\"id\",\n",
    "y_col=\"label\",\n",
    "subset=\"training\",\n",
    "batch_size=32,\n",
    "seed=42,\n",
    "shuffle=True,\n",
    "class_mode=\"categorical\",\n",
    "target_size=(32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "p = np.random.permutation(len(yset))\n",
    "\n",
    "test_train_dict = {}\n",
    "test_train_list = []\n",
    "\n",
    "# Relable for splitting sets\n",
    "Y = []\n",
    "X = []\n",
    "for i in range(0,len(yset)):\n",
    "    Y.append(yset[p[i]])\n",
    "    X.append(img_list[p[i]])\n",
    "    \n",
    "# split the test and training set 75:25\n",
    "split = int(len(X)*(.75))\n",
    "\n",
    "## Original DO NOT DELETE\n",
    "#xtrain = X[:split]\n",
    "#xtest = X[split:]\n",
    "\n",
    "\n",
    "## Testing new \n",
    "xtrain = []\n",
    "xtest = []\n",
    "for i in range(0, len(X)):\n",
    "    \n",
    "    if (i < split):\n",
    "        curr_image_index = matchImage(X[i], img_list)\n",
    "        test_train_dict[nameCol[curr_image_index]] = \"train\"\n",
    "        xtrain.append(X[i])\n",
    "    else:\n",
    "        curr_image_index = matchImage(X[i], img_list)\n",
    "        test_train_dict[nameCol[curr_image_index]] = \"test\"\n",
    "        xtest.append(X[i])\n",
    "\n",
    "    \n",
    "    \n",
    "ytrain = Y[:split]\n",
    "ytest = Y[split:]\n",
    "\n",
    "for i in range(0, len(nameCol)):\n",
    "    #print(i)\n",
    "    #print(nameCol[i])\n",
    "    test_train_list.append(test_train_dict[nameCol[i]])\n",
    "\n",
    "\n",
    "# transform to arrays\n",
    "trainX = np.array(xtrain, dtype=\"float\")/255.0\n",
    "testX = np.array(xtest, dtype =\"float\")/255.0\n",
    "\n",
    "ytrain = np.array(ytrain)\n",
    "ytest = np.array(ytest)\n",
    "\n",
    "# parsed Y data containers\n",
    "trainY = []\n",
    "testY = []\n",
    "\n",
    "\n",
    "# convert labels from int to vectors\n",
    "trainY = np_utils.to_categorical(ytrain,len(uni_labels))\n",
    "testY = np_utils.to_categorical(ytest,len(uni_labels))\n",
    "\n",
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,\n",
    "                        height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
    "                        horizontal_flip=True, fill_mode=\"nearest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.fromarray(np.uint8(cm.gist_earth(myarray)*255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.fromarray(np.uint8(cm.gist_earth(myarray)*255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myarray=trainX[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "\n",
    "model = LeNet.build(width=im_dimension, height=im_dimension, depth=3, classes=len(uni_labels))\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "printmd(\"<b><span style='color:red'>Images split into training and testing sets at 75:25</span></b>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample\n",
    "#train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "#train_generator=train_datagen.flow_from_directory(folder_path,\n",
    "                                                 #target_size=(224,224),\n",
    "                                                 #color_mode='rgb',\n",
    "                                                 #batch_size=32,\n",
    "                                                 #class_mode='categorical',\n",
    "                                                 #shuffle=True)\n",
    "# opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "#model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the predictive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is relative to the size of the data set and the compute resources you use. May take from minutes to hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample\n",
    "# step_size_train=train_generator.n//train_generator.batch_size\n",
    "# model.fit_generator(generator=train_generator,\n",
    "                   #steps_per_epoch=step_size_train,\n",
    "                   #epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "H = model.fit_generator(aug.flow(trainX, trainY, batch_size=BS),\n",
    "    validation_data=(testX, testY), steps_per_epoch=len(trainX) // BS,\n",
    "    epochs=EPOCHS, verbose=1)\n",
    "printmd(\"<b><span style='color:red'>Model generation complete</span></b>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Take the original data and predict based on the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape original input data images for predicting\n",
    "img_check = np.array(img_list, dtype =\"float\")/255.0\n",
    "\n",
    "predictionsMade = []\n",
    "preds = model.predict(img_check)\n",
    "prediction_confidence = []\n",
    "for i in range(0, len(preds)):\n",
    "    prediction_confidence.append(np.amax(preds[i]))   \n",
    "\n",
    "\n",
    "\n",
    "# Run all data through the prediction model that was created\n",
    "for i in range (0,len(img_check)):\n",
    "    predIndex = np.where(preds[i] == np.amax(preds[i]))\n",
    "    prediction = int(predIndex[0][0])\n",
    "    predictionsMade.append(prediction)\n",
    "\n",
    "print(prediction_confidence)    \n",
    "# Count how many correct predictions were made\n",
    "correct = 0\n",
    "for i in range (0,len(predictionsMade)):\n",
    "    if(predictionsMade[i] == yset[i]):\n",
    "        correct += 1 \n",
    "        \n",
    "printmd(\"<b><span style='color:red'>Accuracy: \" + str(correct/len(yset)) + \"</span></b>\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save the model file under models/, and reload it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate model file and save\n",
    "modelName = user + \"_cnn_\" + out2.widget.result + \"_\" + str(epochNum.widget.result) + \"_\" + str(batchNum.widget.result) + \".h5\"\n",
    "modelPath = \"models/\"\n",
    "if not os.path.exists(modelPath):\n",
    "    os.makedirs(modelPath)\n",
    "model.save(os.path.join(modelPath, modelName))\n",
    "#Load model\n",
    "from keras.models import load_model\n",
    "model2 = load_model(os.path.join(modelPath, modelName))\n",
    "printmd(\"<b><span style='color:red'>Model saved</span></b>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Enter a new header for the prediction column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate back to original csv label names\n",
    "finalPred = []\n",
    "for i in range (0,len(predictionsMade)):\n",
    "    finalPred.append(yval_label[predictionsMade[i]])\n",
    "\n",
    "from IPython.display import display\n",
    "input_text = widgets.Text(\n",
    "    value=\"predicted \" + out2.widget.result,\n",
    "    placeholder='Type label here',\n",
    "    disabled=False\n",
    ")\n",
    "output_text = widgets.Text(\n",
    "    value=\"predicted \" + out2.widget.result,\n",
    "    placeholder='New Header will be displayed here',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "def bind_input_to_output(sender):\n",
    "    output_text.value = input_text.value\n",
    "\n",
    "input_text.observe(bind_input_to_output)\n",
    "\n",
    "print(\"Input new column Header Label: \")\n",
    "\n",
    "display(input_text)\n",
    "display(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save the new version of CSV file, and give a name to new survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the new column w/ it's new column name\n",
    "pred_conf_col = \"pred_conf \" + input_text.value\n",
    "test_train_col = \"test_train \" + input_text.value\n",
    "\n",
    "df[input_text.value] = finalPred\n",
    "df[pred_conf_col] = prediction_confidence\n",
    "df[test_train_col] = test_train_list\n",
    "\n",
    "print(input_text.value)\n",
    "\n",
    "new_file = absolutePath + csv_file[:-4]+'_v1.csv'\n",
    "printmd(\"<b><span style='color:red'>A new temporary file will be created at: </span></b>\")\n",
    "print(new_file)\n",
    "df.to_csv(new_file, index=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input survey name\n",
    "\n",
    "default_name = csv_file.split(\".\")[0] + \"_\" + str(EPOCHS) + \"_\" + str(BS) + \"_\" + str(im_dimension)\n",
    "\n",
    "from IPython.display import display\n",
    "input_text = widgets.Text(value=default_name)\n",
    "output_text = widgets.Text(value=default_name)\n",
    "\n",
    "def bind_input_to_output(sender):\n",
    "    output_text.value = input_text.value\n",
    "\n",
    "# Tell the text input widget to call bind_input_to_output() on submit\n",
    "input_text.on_submit(bind_input_to_output)\n",
    "\n",
    "printmd(\"<b><span style='color:red'>Input survey name here, press Enter, and then run the next cell:</span></b>\")\n",
    "# Display input text box widget for input\n",
    "display(input_text)\n",
    "\n",
    "display(output_text)\n",
    "\n",
    "survey_name = output_text.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print survey name\n",
    "survey_name = output_text.value\n",
    "printmd(\"<b><span style='color:red'>Survey Name is: </span></b>\" + survey_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Generate the survey and create survey URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "referer = survey_url.split(\"/main\")[0] +\"/\"\n",
    "upload_url = referer + \"uploadCSV\"\n",
    "new_survey_url_base = survey_url.split(user)[0]\n",
    "\n",
    "import requests\n",
    "import re\n",
    "csv = {\"file\": open(new_file, \"rb\")}\n",
    "upload_data = {\n",
    "    'name': input_text.value,\n",
    "    'dzc': dzc_file,\n",
    "    'user':user\n",
    "}\n",
    "headers = {\n",
    "    'User-Agent': 'suave user agent',\n",
    "    'referer': referer\n",
    "}\n",
    "\n",
    "r = requests.post(upload_url, files=csv, data=upload_data, headers=headers)\n",
    "\n",
    "if r.status_code == 200:\n",
    "    printmd(\"<b><span style='color:red'>New survey created successfully</span></b>\")\n",
    "    regex = re.compile('[^0-9a-zA-Z_]')\n",
    "    s_url = survey_name\n",
    "    s_url =  regex.sub('_', s_url)\n",
    "\n",
    "    url = new_survey_url_base + user + \"_\" + s_url + \".csv\" + \"&views=\" + views + \"&view=\" + view\n",
    "    print(url)\n",
    "    printmd(\"<b><span style='color:red'>Click the URL to open the new survey</span></b>\")\n",
    "else:\n",
    "    printmd(\"<b><span style='color:red'>Error creating new survey. Check if a survey with this name already exists.</span></b>\")\n",
    "    printmd(\"<b><span style='color:red'>Reason: </span></b>\"+ str(r.status_code) + \" \" + r.reason)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
